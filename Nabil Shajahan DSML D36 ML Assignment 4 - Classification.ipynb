{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04b8f251-44c1-4ae0-9124-96e21b9ab803",
   "metadata": {},
   "source": [
    "## ASSIGNMENT 4 - CLASSIFICATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a5f56a-43dc-427a-ab4c-fa70cc31f6a8",
   "metadata": {},
   "source": [
    "**Objective:**\n",
    "\n",
    "The objective of this assessment is to evaluate your understanding and ability to apply supervised learning techniques to a real-world dataset.\n",
    "\n",
    "**Dataset:**\n",
    "\n",
    "Use the breast cancer dataset available in the sklearn library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e6555705-42d7-49c8-99c7-e0bcff7548f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "import sys\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40df9718-4fce-4187-b2b2-998338e73dfc",
   "metadata": {},
   "source": [
    "### **1. Loading and Preprocessing**\n",
    "●\tLoad the breast cancer dataset from sklearn.\n",
    "\n",
    "●\tPreprocess the data to handle any missing values and perform necessary feature scaling.\n",
    "\n",
    "●\tExplain the preprocessing steps you performed and justify why they are necessary for this dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b7c414eb-98b8-4699-907d-50c54d0f9db7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "46c0ad7f-1c2b-435e-ac07-874941420569",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (569, 30)\n",
      "Number of missing values in each column:\n",
      " mean radius                0\n",
      "mean texture               0\n",
      "mean perimeter             0\n",
      "mean area                  0\n",
      "mean smoothness            0\n",
      "mean compactness           0\n",
      "mean concavity             0\n",
      "mean concave points        0\n",
      "mean symmetry              0\n",
      "mean fractal dimension     0\n",
      "radius error               0\n",
      "texture error              0\n",
      "perimeter error            0\n",
      "area error                 0\n",
      "smoothness error           0\n",
      "compactness error          0\n",
      "concavity error            0\n",
      "concave points error       0\n",
      "symmetry error             0\n",
      "fractal dimension error    0\n",
      "worst radius               0\n",
      "worst texture              0\n",
      "worst perimeter            0\n",
      "worst area                 0\n",
      "worst smoothness           0\n",
      "worst compactness          0\n",
      "worst concavity            0\n",
      "worst concave points       0\n",
      "worst symmetry             0\n",
      "worst fractal dimension    0\n",
      "dtype: int64\n",
      "\n",
      "First 5 rows of scaled features:\n",
      "    mean radius  mean texture  mean perimeter  mean area  mean smoothness  \\\n",
      "0     1.097064     -2.073335        1.269934   0.984375         1.568466   \n",
      "1     1.829821     -0.353632        1.685955   1.908708        -0.826962   \n",
      "2     1.579888      0.456187        1.566503   1.558884         0.942210   \n",
      "3    -0.768909      0.253732       -0.592687  -0.764464         3.283553   \n",
      "4     1.750297     -1.151816        1.776573   1.826229         0.280372   \n",
      "\n",
      "   mean compactness  mean concavity  mean concave points  mean symmetry  \\\n",
      "0          3.283515        2.652874             2.532475       2.217515   \n",
      "1         -0.487072       -0.023846             0.548144       0.001392   \n",
      "2          1.052926        1.363478             2.037231       0.939685   \n",
      "3          3.402909        1.915897             1.451707       2.867383   \n",
      "4          0.539340        1.371011             1.428493      -0.009560   \n",
      "\n",
      "   mean fractal dimension  ...  worst radius  worst texture  worst perimeter  \\\n",
      "0                2.255747  ...      1.886690      -1.359293         2.303601   \n",
      "1               -0.868652  ...      1.805927      -0.369203         1.535126   \n",
      "2               -0.398008  ...      1.511870      -0.023974         1.347475   \n",
      "3                4.910919  ...     -0.281464       0.133984        -0.249939   \n",
      "4               -0.562450  ...      1.298575      -1.466770         1.338539   \n",
      "\n",
      "   worst area  worst smoothness  worst compactness  worst concavity  \\\n",
      "0    2.001237          1.307686           2.616665         2.109526   \n",
      "1    1.890489         -0.375612          -0.430444        -0.146749   \n",
      "2    1.456285          0.527407           1.082932         0.854974   \n",
      "3   -0.550021          3.394275           3.893397         1.989588   \n",
      "4    1.220724          0.220556          -0.313395         0.613179   \n",
      "\n",
      "   worst concave points  worst symmetry  worst fractal dimension  \n",
      "0              2.296076        2.750622                 1.937015  \n",
      "1              1.087084       -0.243890                 0.281190  \n",
      "2              1.955000        1.152255                 0.201391  \n",
      "3              2.175786        6.046041                 4.935010  \n",
      "4              0.729259       -0.868353                -0.397100  \n",
      "\n",
      "[5 rows x 30 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the breast cancer dataset\n",
    "data = load_breast_cancer()\n",
    "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
    "y = pd.Series(data.target, name=\"target\")\n",
    "\n",
    "# Display basic info about the dataset\n",
    "print(\"Dataset shape:\", X.shape)\n",
    "print(\"Number of missing values in each column:\\n\", X.isnull().sum())\n",
    "\n",
    "# Preprocessing: Handling missing values\n",
    "# In this dataset, there are no missing values. However, in general, we can handle missing values as follows:\n",
    "# Example: X.fillna(X.mean(), inplace=True)  # Replace missing values with column mean\n",
    "\n",
    "# Preprocessing: Feature scaling\n",
    "scaler = StandardScaler()  # Standardization scales the features to have mean=0 and variance=1\n",
    "X_scaled = scaler.fit_transform(X)  # Fit and transform the features\n",
    "\n",
    "# Splitting the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Verify scaled data\n",
    "print(\"\\nFirst 5 rows of scaled features:\\n\", pd.DataFrame(X_scaled, columns=data.feature_names).head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a4b4c6a-dbdb-4afc-abf6-6efd95349db3",
   "metadata": {},
   "source": [
    "### **2. Classification Algorithm Implementation**\n",
    "\n",
    "●\tImplement the following five classification algorithms:\n",
    "\n",
    "    1. Logistic Regression\n",
    "    2. Decision Tree Classifier\n",
    "    3. Random Forest Classifier\n",
    "    4. Support Vector Machine (SVM)\n",
    "    5. k-Nearest Neighbors (k-NN)\n",
    "\n",
    "●\tFor each algorithm, provide a brief description of how it works and why it might be suitable for this dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b922ad9-aa0c-46b6-b07c-5f5db251bd5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Logistic Regression ===\n",
      "Accuracy: 97.37%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.95      0.96        43\n",
      "           1       0.97      0.99      0.98        71\n",
      "\n",
      "    accuracy                           0.97       114\n",
      "   macro avg       0.97      0.97      0.97       114\n",
      "weighted avg       0.97      0.97      0.97       114\n",
      "\n",
      "\n",
      "=== Decision Tree ===\n",
      "Accuracy: 94.74%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        43\n",
      "           1       0.96      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.94      0.94      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n",
      "\n",
      "=== Random Forest ===\n",
      "Accuracy: 96.49%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.98      0.93      0.95        43\n",
      "           1       0.96      0.99      0.97        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.96      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "\n",
      "=== Support Vector Machine (SVM) ===\n",
      "Accuracy: 95.61%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.95      0.94        43\n",
      "           1       0.97      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.95      0.96      0.95       114\n",
      "weighted avg       0.96      0.96      0.96       114\n",
      "\n",
      "\n",
      "=== k-Nearest Neighbors (k-NN) ===\n",
      "Accuracy: 94.74%\n",
      "Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.93      0.93      0.93        43\n",
      "           1       0.96      0.96      0.96        71\n",
      "\n",
      "    accuracy                           0.95       114\n",
      "   macro avg       0.94      0.94      0.94       114\n",
      "weighted avg       0.95      0.95      0.95       114\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Dictionary to store classifiers and their names\n",
    "classifiers = {\n",
    "    \"Logistic Regression\": LogisticRegression(random_state=42),\n",
    "    \"Decision Tree\": DecisionTreeClassifier(random_state=42),\n",
    "    \"Random Forest\": RandomForestClassifier(random_state=42, n_estimators=100),\n",
    "    \"Support Vector Machine (SVM)\": SVC(kernel='linear', random_state=42),\n",
    "    \"k-Nearest Neighbors (k-NN)\": KNeighborsClassifier(n_neighbors=5)\n",
    "}\n",
    "\n",
    "# Loop through each classifier, train it, and evaluate it\n",
    "for name, clf in classifiers.items():\n",
    "    # Train the classifier\n",
    "    clf.fit(X_train, y_train)\n",
    "    # Make predictions\n",
    "    y_pred = clf.predict(X_test)\n",
    "    # Evaluate performance\n",
    "    print(f\"\\n=== {name} ===\")\n",
    "    print(\"Accuracy: {:.2f}%\".format(accuracy_score(y_test, y_pred) * 100))\n",
    "    print(\"Classification Report:\\n\", classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48cbe379-db0f-4818-b8a7-e279591660ed",
   "metadata": {},
   "source": [
    "### **Explanation of Each Classification Algorithm:**\n",
    "\n",
    "**Logistic Regression:**\n",
    "\n",
    "**How it works:** Logistic regression is a linear model used for binary classification. It calculates the probability of an instance belonging to a class using the sigmoid function and makes predictions based on a decision boundary (typically 0.5).\n",
    "\n",
    "**Suitability:** Since the breast cancer dataset is a binary classification problem (malignant or benign), logistic regression is a strong choice due to its simplicity and effectiveness for linearly separable data.\n",
    "\n",
    "**Decision Tree Classifier:**\n",
    "\n",
    "**How it works:** A decision tree splits the dataset into subsets based on feature values, creating a tree structure. Each node represents a feature, and branches represent decisions leading to leaf nodes (classes).\n",
    "\n",
    "**Suitability:** Decision trees handle non-linear relationships well and are interpretable. They are suitable for datasets with complex interactions between features, as seen in the breast cancer dataset.\n",
    "\n",
    "**Random Forest Classifier:**\n",
    "\n",
    "**How it works:** A random forest is an ensemble method that trains multiple decision trees on random subsets of the data and aggregates their predictions through majority voting.\n",
    "\n",
    "**Suitability:** Random forests reduce overfitting (common in single decision trees) and improve generalization, making them effective for datasets with noise and feature interactions like this one.\n",
    "\n",
    "**Support Vector Machine (SVM):**\n",
    "\n",
    "**How it works:** SVM creates a hyperplane that best separates the data into classes by maximizing the margin between the closest points of each class (support vectors).\n",
    "\n",
    "**Suitability:** SVM works well for datasets with a clear margin of separation between classes and performs robustly with high-dimensional data like the breast cancer dataset.\n",
    "\n",
    "**k-Nearest Neighbors (k-NN):**\n",
    "\n",
    "**How it works:** k-NN classifies an instance based on the majority class among its k nearest neighbors in the feature space.\n",
    "\n",
    "**Suitability:** k-NN is non-parametric and works well for datasets where the decision boundary is not linear. It can capture complex patterns in the breast cancer dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f2d0c2b-5681-4c32-9a4e-af4f9681bdc9",
   "metadata": {},
   "source": [
    "### **3. Model Comparison**\n",
    "\n",
    "●\tCompare the performance of the five classification algorithms.\n",
    "    \n",
    "●\tWhich algorithm performed the best and which one performed the worst?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "328c3200-a60d-4ab0-8ca4-5f632e4d4aea",
   "metadata": {},
   "source": [
    "### **Comparison of Classification Algorithms**\n",
    "\n",
    "**1. Logistic Regression**\n",
    "\n",
    "***Accuracy: 97.37%***\n",
    "\n",
    "Precision: High for both classes (0 = 98%, 1 = 97%)\n",
    "\n",
    "Recall: High for both classes (0 = 95%, 1 = 99%)\n",
    "\n",
    "F1-Score: 96% (Class 0), 98% (Class 1)\n",
    "\n",
    "Observations: Logistic regression performed the best, with the highest accuracy (97.37%) and strong precision, recall, and F1-scores. It is particularly well-suited for this dataset because it works effectively for binary classification problems with linearly separable data.\n",
    "\n",
    "**2. Decision Tree**\n",
    "\n",
    "***Accuracy: 94.74%***\n",
    "\n",
    "Precision: Good for both classes (0 = 93%, 1 = 96%)\n",
    "\n",
    "Recall: Good for both classes (0 = 93%, 1 = 96%)\n",
    "\n",
    "F1-Score: 93% (Class 0), 96% (Class 1)\n",
    "\n",
    "Observations: Decision trees performed decently but had the lowest accuracy (94.74%) among the models. While they are good at capturing non-linear relationships, they are prone to overfitting, which may explain the slightly lower performance compared to other models.\n",
    "\n",
    "**3. Random Forest**\n",
    "\n",
    "***Accuracy: 96.49%***\n",
    "\n",
    "Precision: High for both classes (0 = 98%, 1 = 96%)\n",
    "\n",
    "Recall: High for both classes (0 = 93%, 1 = 99%)\n",
    "\n",
    "F1-Score: 95% (Class 0), 97% (Class 1)\n",
    "\n",
    "Observations: Random forests performed almost as well as logistic regression. They handled feature interactions and reduced overfitting, which helped them achieve high precision and recall. However, their slightly lower recall for Class 0 compared to logistic regression might explain the slightly lower accuracy.\n",
    "\n",
    "**4. Support Vector Machine (SVM)**\n",
    "\n",
    "***Accuracy: 95.61%***\n",
    "\n",
    "Precision: Good for both classes (0 = 93%, 1 = 97%)\n",
    "\n",
    "Recall: Good for both classes (0 = 95%, 1 = 96%)\n",
    "\n",
    "F1-Score: 94% (Class 0), 96% (Class 1)\n",
    "\n",
    "Observations: SVM also performed well, with an accuracy of 95.61%. Its ability to find a decision boundary with a large margin likely contributed to its good performance. However, its accuracy was slightly lower than logistic regression and random forests.\n",
    "\n",
    "**5. k-Nearest Neighbors (k-NN)**\n",
    "\n",
    "***Accuracy: 94.74%***\n",
    "\n",
    "Precision: Good for both classes (0 = 93%, 1 = 96%)\n",
    "\n",
    "Recall: Good for both classes (0 = 93%, 1 = 96%)\n",
    "\n",
    "F1-Score: 93% (Class 0), 96% (Class 1)\n",
    "\n",
    "Observations: k-NN performed similarly to the decision tree, with an accuracy of 94.74%. While it is simple and effective, it may be sensitive to the choice of k and feature scaling. The slightly lower accuracy and F1-scores indicate it might not capture complex patterns as effectively as the other algorithms.\n",
    "\n",
    "### **Best and Worst-Performing Algorithms**\n",
    "\n",
    "**Best-Performing Algorithm: Logistic Regression**\n",
    "\n",
    "Logistic regression achieved the highest accuracy (97.37%) and strong performance across precision, recall, and F1-scores for both classes. This is likely because the dataset is well-suited for linear classification.\n",
    "\n",
    "**Worst-Performing Algorithm: Decision Tree (and k-NN)**\n",
    "\n",
    "Decision tree and k-NN both had the lowest accuracy (94.74%). Decision trees may have suffered from overfitting, while k-NN might have struggled with the optimal k and distance-based classification."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
